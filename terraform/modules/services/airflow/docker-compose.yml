volumes:
  data:

#x-airflow-variables: &airflow-variables
#  AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
#  AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
#  AIRFLOW__CORE__LOAD_EXAMPLES: "False"
#  AIRFLOW__CORE__DAG_IGNORE_FILE_SYNTAX: "glob"
#  AIRFLOW__CORE__DAGS_FOLDER: "/airflow/dags"
#  AIRFLOW__LOGGING__BASE_LOG_FOLDER: "/airflow/logs"
#  AIRFLOW__LOGGING__LOGGING_LEVEL: $LOGLEVEL
#  AIRFLOW_UID: "50000"
#  _AIRFLOW_WWW_USER_PASSWORD: ${PASSWORD}
#  AIRFLOW_CONN_SLACK_API_DEFAULT: ${AIRFLOW_CONN_SLACK_API_DEFAULT}
#  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${DB_URL}
#
#x-common-variables: &common-variables
#  SECRET_KEY: ${SECRET_KEY}
#  ECS_SUBNET: $ECS_SUBNET
#  ECS_SECURITY_GROUP: $ECS_SECURITY_GROUP
#  AWS_DEFAULT_REGION: $AWS_DEFAULT_REGION
#  ENVIRONMENT: $ENVIRONMENT

services:

  source-init:
    image: python:3.10-slim
    container_name: source-init
    entrypoint: >
      bash -c "echo 'Making folders'
      && mkdir -p /airflow
      && mkdir -p /airflow/logs
      && mkdir -p /airflow/dags
      && mkdir -p /airflow/plugins
      && echo 'Making read and write for all'
      && chmod -vR 777 /airflow/logs
      && chmod -vR 777 /airflow/dags"
    volumes:
      - data:/airflow

  scheduler:
    image: apache/airflow:2.10.5
    container_name: airflow-scheduler
    command: scheduler
    restart: always
    ports:
      - "8793:8793"
    environment:
      # <<: [*airflow-variables, *common-variables]
      AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__DAG_IGNORE_FILE_SYNTAX: "glob"
      AIRFLOW__CORE__DAGS_FOLDER: "/airflow/dags"
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: "/airflow/logs"
      AIRFLOW__LOGGING__LOGGING_LEVEL: $LOGLEVEL
      AIRFLOW_UID: "50000"
      _AIRFLOW_WWW_USER_PASSWORD: ${PASSWORD}
      AIRFLOW_CONN_SLACK_API_DEFAULT: ${AIRFLOW_CONN_SLACK_API_DEFAULT}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${DB_URL}
      SECRET_KEY: ${SECRET_KEY}
      ECS_SUBNET: $ECS_SUBNET
      ECS_SECURITY_GROUP: $ECS_SECURITY_GROUP
      AWS_DEFAULT_REGION: $AWS_DEFAULT_REGION
      ENVIRONMENT: $ENVIRONMENT
    user: "${AIRFLOW_UID:-50000}:0"
    volumes:
      - data:/airflow
    # depends_on:
    #  airflowinit:
    #    condition: service_completed_successfully


  webserver:
    image: apache/airflow:2.10.5
    container_name: airflow-webserver
    command: webserver -w 4
    ports:
      - 80:8080
    restart: always
    environment:
      # <<: [*airflow-variables, *common-variables]
      AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__DAG_IGNORE_FILE_SYNTAX: "glob"
      AIRFLOW__CORE__DAGS_FOLDER: "/airflow/dags"
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: "/airflow/logs"
      AIRFLOW__LOGGING__LOGGING_LEVEL: $LOGLEVEL
      AIRFLOW_UID: "50000"
      _AIRFLOW_WWW_USER_PASSWORD: ${PASSWORD}
      AIRFLOW_CONN_SLACK_API_DEFAULT: ${AIRFLOW_CONN_SLACK_API_DEFAULT}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${DB_URL}
      SECRET_KEY: ${SECRET_KEY}
      ECS_SUBNET: $ECS_SUBNET
      ECS_SECURITY_GROUP: $ECS_SECURITY_GROUP
      AWS_DEFAULT_REGION: $AWS_DEFAULT_REGION
      ENVIRONMENT: $ENVIRONMENT
      AIRFLOW__WEBSERVER__SECRET_KEY: ${SECRET_KEY}
      FORWARDED_ALLOW_IPS: "*"
      AIRFLOW__WEBSERVER__WORKER_CLASS: "gevent"
      _AIRFLOW_PATCH_GEVENT: "1"
    user: "${AIRFLOW_UID:-50000}:0"
    volumes:
      - data:/airflow
    # depends_on:
    #   airflowinit:
    #     condition: service_completed_successfully


  # only need to (and only able to!) run this once. Commit ref 96b5bb8 has this uncommented.
  # Make sure create a database called "airflow" first!
  #  airflowinit:
  #    image: apache/airflow:2.10.5
  #    container_name: airflow-init
  #    environment:
  #      <<: [*airflow-variables, *common-variables]
  #      _AIRFLOW_DB_UPGRADE: 'True'
  #      _AIRFLOW_WWW_USER_CREATE: 'True'
  #      _AIRFLOW_WWW_USER_USERNAME: 'airflow'
  #      _AIRFLOW_WWW_USER_PASSWORD: ${PASSWORD}
  #    user: "${AIRFLOW_UID:-50000}:0"
  #    command: >
  #      bash -c "echo 'Installing dependencies'
  #      && pip install --quiet apache-airflow[amazon]
  #      && echo 'Making directories'
  #      && mkdir -p /airflow/logs /airflow/dags /airflow/plugins
  #      && chmod -v 777 /airflow/{logs,dags}
  #      && echo 'Initializing database'
  #      && airflow db init -v"
  #    volumes:
  #      - data:/airflow

  sync-s3:
    image: amazon/aws-cli
    container_name: sync-s3
    entrypoint: >
      bash -c "while true; aws s3 sync --exact-timestamps --delete 's3://${BUCKET}/dags' '/airflow/dags';
      mkdir -p /airflow/{logs,dags}; 
      chmod -R 777 /airflow/{logs,dags}; do sleep 2; done;"
    volumes:
      - data:/airflow
    environment:
      AWS_DEFAULT_REGION: $AWS_DEFAULT_REGION
      ENVIRONMENT: $ENVIRONMENT
      BUCKET: $BUCKET
    restart: always
    # depends_on:
    #   airflowinit:
    #     condition: service_completed_successfully

