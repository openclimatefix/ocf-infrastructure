version: "3"

services:
  # TODO remove and use postgres
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5434:5432"

  source-init:
    image: apache/airflow:2.6.2
    container_name: source-init
    entrypoint: >
      bash -c "echo 'Making folders'
      && mkdir -p sources
      && mkdir -p sources/logs
      && mkdir -p sources/dags
      && mkdir -p sources/plugins
      && echo 'Making read and write for all'
      && chmod -R 777 /sources"
    volumes:
      - .:/sources

#
  scheduler:
    depends_on:
      - "postgres"
      - "airflowinit"
    image: apache/airflow:2.6.2
    container_name: airflow-scheduler
    command: scheduler
    restart: on-failure
    ports:
      - "8793:8793"
    environment:
      AIRFLOW__CORE__FERNET_KEY: "" # TODO envvar
      SECRET_KEY: "" # TODO envvar
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "True" # TODO remove
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres/airflow"
      AIRFLOW_UID: "5000"
      AIRFLOW__CORE__DAGS_FOLDER: "/airflow/dags"
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: "/airflow/logs"
      AIRFLOW__LOGGING__LOGGING_LEVEL: "INFO" # TODO envvar
      # AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY
      # AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID
      AWS_DEFAULT_REGION: $AWS_DEFAULT_REGION
    user: "${AIRFLOW_UID:-50000}:0"
    volumes:
      - ./dags:/airflow/dags
      - ./logs:/airflow/logs
      - ./plugins:/airflow/plugins

  webserver:
    image: apache/airflow:2.6.2
    container_name: airflow-webserver
    command: webserver -w 1
    depends_on:
      - "postgres"
      - "airflowinit"
    ports:
      - 80:8080
    restart: always
    environment:
      AIRFLOW__CORE__FERNET_KEY: "" # TODO envvar
      SECRET_KEY: "" # TODO envvar
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "True" # TODO remove
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres/airflow"
      FORWARDED_ALLOW_IPS: "*"
      AIRFLOW__WEBSERVER__WORKER_CLASS: "gevent"
      _AIRFLOW_PATCH_GEVENT: "1"
      AIRFLOW_UID: "5000"
      AIRFLOW__CORE__DAGS_FOLDER: "/airflow/dags"
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: "/airflow/logs"
      # AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY
      # AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID
      AWS_DEFAULT_REGION: $AWS_DEFAULT_REGION
    user: "${AIRFLOW_UID:-50000}:0"
    volumes:
      - ./dags:/airflow/dags
      - ./logs:/airflow/logs
      - ./plugins:/airflow/plugins

  airflowinit:
    image: apache/airflow:2.6.2
    container_name: airflow-init
    depends_on: ["postgres"]
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres/airflow"
      _AIRFLOW_DB_UPGRADE: 'True'
      _AIRFLOW_WWW_USER_CREATE: 'True'
      _AIRFLOW_WWW_USER_USERNAME: 'airflow'
      _AIRFLOW_WWW_USER_PASSWORD: 'airflow'# TODO envvar
      AIRFLOW_UID: "5000"
    user: "0:0"
    command: >
      bash -c "pip install apache-airflow[amazon]
      && airflow db init"

  sync-s3:
    image: amazon/aws-cli
    container_name: sync-s3
    entrypoint: >
      bash -c "while true; aws s3 sync --exact-timestamps --delete 's3://ocf-airflow-testing-bucket/dags' '/sources/dags';
      do sleep 2; done;"
    volumes:
      - .:/sources
    environment:
      # AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY
      # AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID
      AWS_DEFAULT_REGION: $AWS_DEFAULT_REGION
    restart: always
    depends_on:
      - "postgres"
      - "airflowinit"
