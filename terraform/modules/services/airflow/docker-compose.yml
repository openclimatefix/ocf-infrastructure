version: "3"


volumes:
  data:

services:
  # TODO remove and use postgres
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5434:5432"

  source-init:
    image: python:3.10-slim
    container_name: source-init
    entrypoint: >
      bash -c "echo 'Making folders'
      && mkdir -p /airflow
      && mkdir -p /airflow/logs
      && mkdir -p /airflow/dags
      && mkdir -p /airflow/plugins
      && echo 'Making read and write for all'
      && chmod -vR 777 /airflow/logs
      && chmod -vR 777 /airflow/dags"
    volumes:
      - data:/airflow

#
  scheduler:
    depends_on:
      - "postgres"
      - "airflowinit"
    image: apache/airflow:2.6.2
    container_name: airflow-scheduler
    command: scheduler
    restart: on-failure
    ports:
      - "8793:8793"
    environment:
      AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
      SECRET_KEY: ${SECRET_KEY}
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "True" # TODO remove
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres/airflow"
      AIRFLOW_UID: "50000"
      AIRFLOW__CORE__DAGS_FOLDER: "/airflow/dags"
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: "/airflow/logs"
      AIRFLOW__LOGGING__LOGGING_LEVEL: $LOGLEVEL
      AWS_DEFAULT_REGION: $AWS_DEFAULT_REGION
    user: "${AIRFLOW_UID:-50000}:0"
    volumes:
      - data:/airflow


  webserver:
    image: apache/airflow:2.6.2
    container_name: airflow-webserver
    command: webserver -w 1
    depends_on:
      - "postgres"
      - "airflowinit"
    ports:
      - 80:8080
    restart: always
    environment:
      AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
      SECRET_KEY: ${SECRET_KEY}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${SECRET_KEY}
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "True" # TODO remove
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres/airflow"
      FORWARDED_ALLOW_IPS: "*"
      AIRFLOW__WEBSERVER__WORKER_CLASS: "gevent"
      _AIRFLOW_PATCH_GEVENT: "1"
      AIRFLOW_UID: "50000"
      AIRFLOW__CORE__DAGS_FOLDER: "/airflow/dags"
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: "/airflow/logs"
      AWS_DEFAULT_REGION: $AWS_DEFAULT_REGION
    user: "${AIRFLOW_UID:-50000}:0"
    volumes:
      - data:/airflow

  airflowinit:
    image: apache/airflow:2.6.2
    container_name: airflow-init
    depends_on: ["postgres"]
    environment:
      AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
      SECRET_KEY: ${SECRET_KEY}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${SECRET_KEY}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres/airflow"
      _AIRFLOW_DB_UPGRADE: 'True'
      _AIRFLOW_WWW_USER_CREATE: 'True'
      _AIRFLOW_WWW_USER_USERNAME: 'airflow'
      _AIRFLOW_WWW_USER_PASSWORD: ${PASSWORD}
      AIRFLOW__CORE__DAGS_FOLDER: "/airflow/dags"
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: "/airflow/logs"
      AIRFLOW_UID: "50000"
    user: "${AIRFLOW_UID:-50000}:0"
    command: >
      bash -c "pip install apache-airflow[amazon]
      && mkdir -p /airflow/logs /airflow/dags /airflow/plugins
      && chmod -vR 777 /airflow/{logs,dags}
      && airflow db init"
    volumes:
      - data:/airflow


  sync-s3:
    image: amazon/aws-cli
    container_name: sync-s3
    entrypoint: >
      bash -c "while true; aws s3 sync --exact-timestamps --delete 's3://ocf-airflow-${ENVIRONMENT}-bucket/dags' '/airflow/dags';
      mkdir -p /airflow/{logs,dags}; 
      chmod -vR 777 /airflow/{logs,dags}; do sleep 2; done;"
    volumes:
      - data:/airflow
    environment:
      AWS_DEFAULT_REGION: $AWS_DEFAULT_REGION
      ENVIRONMENT: $ENVIRONMENT
    restart: always
    depends_on:
      - "postgres"
      - "airflowinit"
